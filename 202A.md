# 202A Notes

## Chapter 1 Probability Theory

### 1.1 Set Theory

**Def 1.1.1 (Sample space)**
The set, $S$, of all possible outcome of a particular experiment is called the sample space for the experiment.

**Def 1.1.2 (Event)**
An event is any collection of possible outcomes of an experiment, that is, any subset of $S$ (including $S$ itself).

**Th 1.1.4 (Properties of sets)**
For any three events, A, B, and C, define on a sample space $S$,
1. Commutativity:
$A \cup B=B \cup A$; $A \cap B=B \cap A$
2. Associativity: 
$A \cup (B \cup C)=(A \cup B) \cup C$; $A \cap (B \cap C)=(A \cap B) \cap C$
3. Distributive Laws: 
$A \cap (B\cup C)=(A \cap B) \cup (A \cap C)$; $A \cup (B\cap C)=(A \cup B) \cap (A \cup C)$
4. DeMorgan's Laws: 
$(A \cup B)^c = A^c \cap B^c$; $(A \cap B)^c = A^c \cup B^c$

**Def 1.1.5 (Disjoint sets)**
Two events A and B are disjoint (or mutually exclusive) if $A \cap B=\emptyset$. The events $A_1,A_2,\dots$ are pairwise disjoint (or mutually exclusive) if $A_i \cap A_j = \emptyset$ for all $i \neq j$.

**Def 1.1.6 (Partition)**
If $A_1,A_2,\dots$ are pairwise disjoint and $\cup_{i=1}^{\infty}=S$, then the collection $A_1,A_2,\dots$ forms a partition of $S$.

### 1.2 Basics of Probablility Theory

**Def 1.2.1 ($\sigma$-Algebra)**
A collection of subsets of $S$ is called a sigma algebra (or Borel field), denoted by $\mathcal B$, if it satisfies the following three properties: 
1. $\emptyset \in \mathcal B$
2. If $A \in \mathcal B$, then $A^c \in \mathcal B$ ($\mathcal B$ is closed under complementation)
3. If $A_1, A_2, \dots \in \mathcal B$, then $\cup_{i=1}^{\infty} A_i \in \mathcal B$ ($\mathcal B$ is closed under countable unions)

**Def 1.2.4 (Probability function)**
Given a sample space $S$ and an associated sigma algebra $\mathcal B$, a probability function is a function $P$ with domain $\mathcal B$ that satisfies: 
1. $P(A) \geq 0$ for all $A \in \mathcal B$
2. $P(S)=1$
3. If $A_1, A_2, \dots \in \mathcal B$ are pairwise disjoint, then $P(\cup_{i=1}^{\infty} A_i)= \sum_{i=1}^{\infty}{P(A_i)}$

**Th 1.2.6 (Simple definition of probability function)**
Let $S=\{s_1, \dots, s_n\}$ be a finite set. Let $\mathcal B$ be any sigma algebra of subsets of $S$. Let $p_1,\dots, p_n$ be nonnegative numbers that sum to 1. For any $A\in \mathcal B$, define $P(A)$ by
$$P(A)=\sum_{i:s_i\in A}p_i.$$
(The sum over an empty set is defined to be 0.) Then P is a probability function on $\mathcal B$. This remains true if $S=\{s_1,s_2,\dots\}$ is a countable set. 

**Th 1.2.8 (Properties of the probability funciton I)**
1. $P(\emptyset)=0$
2. $P(A)\leq 1$
3. $P(A^c)=1-P(A)$

**Th 1.2.9 (Properties of the probability funciton II)**
If $P$ is a probability function and $A$ and $B$ are any sets in $\mathcal B$, then
1. $P(B \cap A^c)=P(B)-P(A \cap B)$
2. $P(A \cup B)=P(A) + P(B) - P(A\cap B)$
3. If $A \subset B$, then $P(A) < P(B)$

**Th 1.2.10 (Bonferroni's inequality)**
$$P(A\cap B)\geq P(A)+P(B)-1$$
$$P\left(\bigcap_{i=1}^{n}A_i\right) \geq\sum_{i=1}^n P(A_i)-(n-1)$$

**Th 1.2.11 (Results for dealing with a collection of sets)**
If $P$ is a probability function, then
1. $P(A)=\sum_{i=1}^{\infty}{P(A\cap C_i)}$ for any partition $C_1,C_2,\dots$
2. $P(\cup_{i=1}^{\infty}A_i)\leq \sum_{i=1}^{\infty}{P(A_i)}$ for any sets $A_1, A_2, \dots$ (Boole's inequality)


*Proof for 2:*
We first establish a disjoint collection $A_1^*,A_2^*,\dots$, with the property that $\cup_{i=1}^{\infty}A_i^*=\cup_{i=1}^{\infty}A_i$.

$$
A_1^{*}=A_1,\quad A_i^*=A_i \backslash \left(\bigcup_{j=1}^{i-1} A_j \right),i=2,3,\dots
$$

Therefore, we have

$$
P\left( \bigcup_{i=1}^{\infty} A_i \right)=P\left( \bigcup_{i=1}^{\infty} A_i^* \right)=\sum_{i=1}^{\infty}P(A_i^*)
$$

where the last equality follows since the $A_i^*$ are disjoint. To see this, we write

$$
\begin{aligned}
A_i^* \cap A_k^* &= \left\{ A_i \backslash \left( \bigcup_{j=1}^{i-1} A_j\right)\right\} \cap \left\{  A_k \backslash \left( \bigcup_{j=1}^{k-1} A_j\right)\right\} \text{(definition of $A_i^*$)}\\
&=\left\{ A_i \cap \left( \bigcup_{j=1}^{i-1} A_j\right)^c\right\} \cap \left\{ A_k \cap \left( \bigcup_{j=1}^{k-1} A_j\right)^c\right\} \text{(definition of $\backslash$ )}\\
&=\left\{ A_i \cap \bigcap_{j=1}^{i-1}A_j^c\right\} \cap \left\{ A_k \cap \bigcap_{j=1}^{k-1}A_j^c\right\} \text{(DeMorgan's Laws)}
\end{aligned}
$$

Now if $i>k$, the first intersection above will be contained in the set $A_k^c$, which will have an empty intersection with $A_k$. If $k>i$, the argument is similar. Further, by construction $A_i^* \subset A_i$, so $P(A_i^*)\leq P(A_i)$ and we have

$$\sum_{i=1}^{\infty}P(A_i^*) \leq \sum_{i=1}^{\infty}P(A_i)$$


**Th 1.2.14 (Fundamental theorem of counting)**
If a job consists of $k$ seperate tasks, the i-th of which can be done in $n_i$ ways, then the entire job can be done in $n_1 \times n_2 \times \dots \times n_k$ ways.

**Def 1.2.17 (Combination)**
For nonnegative integers $n$ and $r$, where $n \geq r$, we define the symbol ${{n}\choose{r}}=\frac{n!}{r!(n-r)!}$

**Remark 1.2.18 (Number of possible arrangements of size $r$ from $n$ objects)**
1. Ordered, without replacement: $\frac{n}{(n-r)!}$
2. Ordered, with replacement: $n^r$
3. Unordered, without replacement: ${n}\choose{r}$
4. Unordered, with replacement: ${n+r-1}\choose{r}$

**Remark 1.2.19 (Methods for poker game)**
If we wish to calculate probabilities for events that depend on the order, such as the probability of an ace in the first two cards, then we must use the ordered outcomes. If we want to calculate the probability of an event that does not depend on the order, we can use either the ordered or unordered sample space.

**Remark 1.2.20 (Sampling with replacement)**
Consider sampling $r=2$ items from $n=3$ items, with replacement.

|  Unordered  | {1,1} | {2,2} | {3,3} |    {1,2}    |    {1,3}    |    {2,3}    |
|:-----------:|:-----:|:-----:|:-----:|:-----------:|:-----------:|:-----------:|
|   Ordered   | (1,1) | (2,2) | (3,3) | (1,2),(2,1) | (1,3),(3,1) | (2,3),(3,2) |
| Probability |  1/9  |  1/9  |  1/9  |     2/9     |     2/9     |     2/9     |

The formula for the number of outcomes in the unordered sample space is useful for enumerating the outcomes, but ordered outcomes must be counted to correctly calculate probabilities.

### 1.3 Conditional Probability and Independence

**Def 1.3.2 (Conditional probability)**
If $A$ and $B$ are events in $S$, and $P(B)>0$, then the conditional probability of $A$ given $B$, written $P(A|B)=\frac{P(A\cap B)}{P(B)}$

**Th 1.3.5 (Bayes' rule)**
Let $A_1,A_2,\dots$ be a partition of the sample space, and let $B$ be any set. Then, for each $i=1,2,\dots$, 
$$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{\infty}{P(B|A_j)P(A_j)}}$$

**Def 1.3.7 (Independence)**
Two events, $A$ and $B$, are statistically independent if $P(A \cap B) = P(A) P(B)$

**Th 1.3.9 (Independence)**
If $A$ and $B$ are independent events, then the following pairs are also independent: 
1. $A$ and $B^c$
2. $A^c$ and $B$
3. $A^c$ and $B^c$

**Remark 1.3.8 (Independence among more than two events)**

The requirement $P(A \cap B \cap C)=P(A)P(B)P(C)$ is NOT a strong enough condition to garantee pairwise independence.

If $A,B,C$ are pairwise independent, there might be $P(A \cap B \cap C) \neq P(A)P(B)P(C)$.

**Def 1.3.12 (Mutually independent)**
A collection of events $A_1,\dots,A_n$ are mutually independent if for any subcollection $A_{i_1},\dots,A_{i_k}$, we have
$$P(\bigcap_{j=1}^k A_{i_j})= \prod_{j=1}^{k} P(A_{i_j})$$


### 1.4 Random Variables

**Def 1.4.1 (Random variables)**
A random variable is a function from a sample space $S$ into real numbers.

**Remark 1.4.2 (Random variables)**
In defining a random variable, we have also define a new sample space (the range of the random variable). We must check formally that our probability function, which is defined on the original sample space, can be used for the random variable.

Suppose we have a sample space $S=\{s_1,\dots,s_n\}$ with a probability function $P$, and we define a random variable $X$ with range $\mathcal X=\{x_1, \dots, x_m\}$. We can define a probability function $P_X$ on $\mathcal X$ in the following way.

$$P_X(X=x_i)=P(\{s_j \in S: X(s_j)=x_i\})$$

Such is also the case if $\mathcal X$  is countable. If $\mathcal X$ is uncountable, we define in the following way. For any set $A \subset \mathcal X$,
$$P_X(X \in A) = P(\{s \in S : X(s) \in A \})$$

### 1.5 Distribution Functions

**Def 1.5.1 (Cummulative distribution function)**
The cumulative distribution function or cdf of a random variable X, denoted by $F_X(x)$, is defined by
$$F_X(x)=P_X(X \leq x), \quad \text{for all }x$$

